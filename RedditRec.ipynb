{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c29f1a4-b286-451c-9a67-0204f73a2564",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070bfcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize, bigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from tqdm import tqdm\n",
    "# import praw\n",
    "# from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb184edf-2abb-41e4-a3a9-b6c09e6f8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "# display(HTML('<style>div.output_scroll { height: 44em; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af921409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\CS Dev\\CS\n",
      "[nltk_data]     Stuff\\Projects\\Scripts\\RedditRec\\venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to D:\\CS Dev\\CS\n",
      "[nltk_data]     Stuff\\Projects\\Scripts\\RedditRec\\venv\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:\\CS Dev\\CS\n",
      "[nltk_data]     Stuff\\Projects\\Scripts\\RedditRec\\venv\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_data_dir = os.path.join(os.getcwd(), 'venv', 'nltk_data')\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_dir)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ec04d5-873a-4243-b61b-7a405e87dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62080002-2a20-45b9-a7fa-b079e359a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7f8ea9-0ae6-4653-96df-a0c7d5275972",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1671793-a010-4de0-997e-dc4b7f8b52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = spacy_stop_words.union(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e75cb-03f1-47c7-bda2-b5dc55269b79",
   "metadata": {},
   "source": [
    "### 2. Fetch Reddit JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ba35df-da67-4c15-9dfd-2453f609a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/kdramarecommends/comments/17ehrue/best_kdramas/'\n",
    "# url = 'https://www.reddit.com/r/televisionsuggestions/comments/1cfrjwa/must_watch_tv_shows/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace48f03-2bb4-4b78-bdf8-04e3da3dd83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched JSON data!\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url + '.json', headers=headers) # Add JSON tag to properly extract all website data. Scraping does not work.\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print('Successfully fetched JSON data!')\n",
    "else:\n",
    "    print('Failed to fetch data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d60a1ab-277a-4f99-94a4-8e322f0b320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63861cdd-93e8-485b-89e0-90f43361b2dc",
   "metadata": {},
   "source": [
    "### 3. Extract Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ca4a538-5ba1-4682-8b4c-9e8bbf7f7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_comments(json_data, comments_list):\n",
    "    \"\"\"Recursively traverses JSON to extract all 'body' texts and their corresponding 'score' values.\"\"\"\n",
    "    if isinstance(json_data, dict):\n",
    "        # If the current object has a 'body' and 'score', we add them to our list\n",
    "        if 'body' in json_data and 'score' in json_data:\n",
    "            comment_body = json_data['body']\n",
    "            score = json_data['score']\n",
    "            comments_list.append({'text': comment_body, 'score': score})\n",
    "\n",
    "        # Recursively check each key-value pair in the dictionary\n",
    "        for key, value in json_data.items():\n",
    "            extract_all_comments(value, comments_list)\n",
    "\n",
    "    elif isinstance(json_data, list):\n",
    "        # If it's a list, apply the extraction function to each item\n",
    "        for item in json_data:\n",
    "            extract_all_comments(item, comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f676e58c-8fa7-4945-8af3-d645826a4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "extract_all_comments(data, comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9985bc7c-6b09-410e-ae57-8115fde3426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebe9f63-35f9-44e0-aeef-136d06d18a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment in comments:\n",
    "    comment['text'] = clean_text(comment['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0ae9ba9-8ce3-42f1-8c73-e88b32670491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments extracted: 88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Total comments extracted: {len(comments)}\\n')\n",
    "\n",
    "# for comment in comments:\n",
    "#     print(f'{comment}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec15d6-ab1e-4f65-bacc-ee1fbbf8ab60",
   "metadata": {},
   "source": [
    "### 4. Calculate TF-IDF Scores for Upvote Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a7c0064-6f3d-483f-8f92-dac58bcecbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_t = [comment['text'] for comment in comments]\n",
    "comment_s = [comment['score'] for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a4a2de7-ea7a-4197-b091-25a67c55f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a913c887-2d70-460f-ada4-10912990a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CS Dev\\CS Stuff\\Projects\\Scripts\\RedditRec\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "tfidf_mx = vectorizer.fit_transform(comment_t)\n",
    "w_tfidf_mx = tfidf_mx.multiply(np.array(comment_s).reshape(-1, 1))\n",
    "\n",
    "# print(w_tfidf_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d824d6d-8968-431a-a2e1-8d1e6c11738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Terms Weighted by Upvotes and TF-IDF:\n",
      "crash: 18.948682870281164\n",
      "landing: 18.591948522936583\n",
      "love: 18.543791923322075\n",
      "adore: 17.285165707210997\n",
      "blindly: 17.285165707210997\n",
      "forever: 17.285165707210997\n",
      "beginner: 16.781174297886682\n",
      "mr: 14.92813530519487\n",
      "mister: 13.271694012395832\n",
      "goblin: 10.53226674261094\n"
     ]
    }
   ],
   "source": [
    "term_scores = w_tfidf_mx.sum(axis=0)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "term_scores_dict = {terms[i]: term_scores[0, i] for i in range(len(terms))}\n",
    "sorted_terms = sorted(term_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('\\nTop 10 Terms Weighted by Upvotes and TF-IDF:')\n",
    "for term, score in sorted_terms[:10]:\n",
    "    print(f'{term}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a7cd8-ba0f-4d2c-bf43-d008525c262e",
   "metadata": {},
   "source": [
    "### 5. Co-occurrence Detection for Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "066cfc83-9f27-47f2-bf4c-69d9cb14e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_named_entities(comments):\n",
    "#     entity_counts = defaultdict(Counter)\n",
    "#     all_entities = []\n",
    "    \n",
    "#     for comment in tqdm(comments, desc='Extracting named entities'):\n",
    "#         doc = nlp(comment['text'])\n",
    "        \n",
    "#         for ent in doc.ents:\n",
    "#             entity_text = ent.text.lower()\n",
    "#             entity_type = ent.label_\n",
    "            \n",
    "#             entity_counts[entity_type][entity_text] += comment['score']\n",
    "#             all_entities.append({\n",
    "#                 'text': entity_text,\n",
    "#                 'type': entity_type,\n",
    "#                 'score': comment['score']\n",
    "#             })\n",
    "    \n",
    "#     return entity_counts, all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3f84b9d-886b-4a45-8b13-5c7f94cadbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_counts, all_entities = extract_named_entities(comments)\n",
    "\n",
    "# print('\\nEntity type distribution:')\n",
    "# for entity_type, counts in entity_counts.items():\n",
    "#     print(f'\\n{entity_type}:')\n",
    "#     sorted_entities = sorted(counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "#     for entity, score in sorted_entities:\n",
    "#         print(f'  - {entity}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b54e4b3-24a6-45df-a58a-2a941f04161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_relevant_entities(entity_counts, relevant_types=None):\n",
    "#     if relevant_types is None:\n",
    "#         relevant_types = {'WORK_OF_ART', 'PERSON', 'PRODUCT', 'EVENT'}\n",
    "    \n",
    "#     filtered_entities = {}\n",
    "#     for etype, counts in entity_counts.items():\n",
    "#         if etype in relevant_types:\n",
    "#             filtered_entities[etype] = counts\n",
    "    \n",
    "#     return filtered_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8d00c54-d899-418c-ab6c-60ce3a813964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_entities = filter_relevant_entities(entity_counts)\n",
    "\n",
    "# print('\\nFiltered relevant entities:')\n",
    "# for entity_type, counts in filtered_entities.items():\n",
    "#     print(f'\\n{entity_type}:')\n",
    "#     sorted_entities = sorted(counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "#     for entity, score in sorted_entities:\n",
    "#         print(f'  - {entity}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdde23fd-c6be-4a7d-b2bc-a337391688e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def verify_phrases_with_ner(phrases, entities, threshold=0.3):\n",
    "#     verified_phrases = []\n",
    "#     entity_texts = [e['text'] for e in entities]\n",
    "    \n",
    "#     for phrase in phrases:\n",
    "#         phrase_doc = nlp(phrase.lower())\n",
    "        \n",
    "#         if phrase.lower() in entity_texts:\n",
    "#             verified_phrases.append(phrase)\n",
    "#             continue\n",
    "            \n",
    "#         for entity in entities:\n",
    "#             entity_doc = nlp(entity['text'])\n",
    "#             if phrase_doc.similarity(entity_doc) > threshold:\n",
    "#                 verified_phrases.append(phrase)\n",
    "#                 break\n",
    "    \n",
    "#     return verified_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "add45665-9a50-4c2b-ac4d-4326be31d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_common_phrases(comments, ngram_limit=10, min_occurrences=3):\n",
    "    ngram_list = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        tokens = nltk.word_tokenize(comment['text'])\n",
    "        \n",
    "        for n in range(2, ngram_limit + 1):\n",
    "            for ngram in ngrams(tokens, n):\n",
    "                if not all(word.lower() in stop_words for word in ngram):\n",
    "                    ngram_list.append(ngram)\n",
    "    \n",
    "    ngram_counts = Counter(ngram_list)\n",
    "    common_phrases = [ngram for ngram, count in ngram_counts.items() if count >= min_occurrences]\n",
    "    \n",
    "    return common_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "028e503e-d229-4adf-ad44-baeb19cd0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_to_string(phrase_tuple):\n",
    "    \"\"\"Convert a tuple of words into a single string.\"\"\"\n",
    "    return ' '.join(phrase_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fa4d0cc-dd4e-4735-be9d-f0f921738986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_meaningful_phrase(phrase):\n",
    "    \"\"\"Retain phrases with at least 2 capitalized words.\"\"\"\n",
    "    capitalized_count = sum(1 for word in phrase if word[0].isupper())\n",
    "    return capitalized_count >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73f5acc9-068c-4c3d-9070-f060172c891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_substrings(phrases):\n",
    "    \"\"\"Remove phrases that are substrings of other phrases.\"\"\"\n",
    "    final_phrases = set()\n",
    "    sorted_phrases = sorted(phrases, key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    for phrase in sorted_phrases:\n",
    "        if not any(phrase.lower() in other.lower() for other in final_phrases):\n",
    "            final_phrases.add(phrase)\n",
    "    \n",
    "    return final_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "956c715a-b2e2-40ba-87e0-c915336c84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_grammar(phrases):\n",
    "    \"\"\"Remove leading and trailing grammar words from a list of phrases and remove leading numbers in the form 'X '.\"\"\"\n",
    "    grammar = {'a', 'an', 'the', 'i'}\n",
    "    processed_phrases = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        words = phrase.split()\n",
    "\n",
    "        if words and re.match(r'^\\d+$', words[0]):\n",
    "            words = words[1:]\n",
    "\n",
    "        # Remove leading grammar words\n",
    "        if words and words[0].lower() in grammar:\n",
    "            words = words[1:]\n",
    "\n",
    "        # Remove trailing grammar words\n",
    "        if words and words[-1].lower() in grammar:\n",
    "            words = words[:-1]\n",
    "\n",
    "        processed_phrases.append(' '.join(words))\n",
    "\n",
    "    return processed_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff514507-4397-4a67-be4f-bf6428d5c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_custom_words(phrase, custom_words):\n",
    "    \"\"\"Remove custom words from the phrase.\"\"\"\n",
    "    words = phrase.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in custom_words]\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dd70ee6-e807-44ba-9e30-6292114a954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_only_phrases(phrases, stop_words):\n",
    "    \"\"\"Remove phrases that consist entirely of stopwords or are too short.\"\"\"\n",
    "    filtered_phrases = []\n",
    "    for phrase in phrases:\n",
    "        words = phrase.split()\n",
    "        if not all(word.lower() in stop_words for word in words) and len(words) > 2:\n",
    "            filtered_phrases.append(phrase)\n",
    "    return filtered_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26e0d568-4fb8-4062-8f31-64dfa74c1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_lowercase_phrases(phrases):\n",
    "    \"\"\"Remove phrases that consist entirely of lowercase words or start with common non-informative words.\"\"\"\n",
    "    articles_and_common_words = {\n",
    "        'A', 'An', 'The', 'And', 'But', 'Or', 'So', 'Because', 'However', 'If', 'In', 'On', 'At', \n",
    "        'For', 'By', 'To', 'From', 'With', 'About', 'Over', 'Under', 'Before', 'After', \n",
    "        'I', 'Ive', 'He', 'Hes', 'She', 'Shes', 'It', 'Its', 'They', 'Theyve', 'We', 'Weve', 'This', 'That', 'These', 'Those', 'Then', \n",
    "        'Now', 'Here', 'There', 'What', 'When', 'Where', 'Why', 'How', 'Who', 'Which'\n",
    "    }\n",
    "    \n",
    "    filtered_phrases = []\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        words = phrase.split()\n",
    "        capitalized_words = [word for word in words if word[0].isupper() and word not in articles_and_common_words]\n",
    "\n",
    "        if capitalized_words:\n",
    "            filtered_phrases.append(phrase)\n",
    "    \n",
    "    return filtered_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff9dbd42-328d-4ea2-83cd-92742c9833a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_ngrams(phrases):\n",
    "    \"\"\"Process n-grams to remove duplicates, check capitalization, and filter based on length.\"\"\"\n",
    "    phrases = [tuple_to_string(phrase) for phrase in phrases]\n",
    "    processed_phrases = []\n",
    "    seen_phrases = set()\n",
    "\n",
    "    for phrase in phrases:\n",
    "        phrase_lower = ' '.join(word.lower() for word in phrase.split())\n",
    "\n",
    "        if phrase_lower not in seen_phrases:\n",
    "            processed_phrases.append(phrase)\n",
    "            seen_phrases.add(phrase_lower)\n",
    "\n",
    "    final_phrases = set()\n",
    "    for phrase in processed_phrases:\n",
    "        phrase_str = phrase\n",
    "\n",
    "        if is_meaningful_phrase(phrase_str.split()) or len(phrase_str.split()) > 2:\n",
    "            final_phrases.add(phrase_str)\n",
    "\n",
    "    return final_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5209b849-8928-4934-9482-8ce59f4ecd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_similar_phrases(phrases, similarity_threshold=3):\n",
    "    \"\"\"Combine phrases that have similar starts or ends.\"\"\"\n",
    "    combined_phrases = list(phrases)\n",
    "    merged = True\n",
    "    \n",
    "    while merged:\n",
    "        merged = False\n",
    "        new_phrases = []\n",
    "        skip = set()\n",
    "\n",
    "        for i, phrase in enumerate(combined_phrases):\n",
    "            if i in skip:\n",
    "                continue\n",
    "\n",
    "            phrase_words = phrase.split()\n",
    "            combined_phrase = phrase\n",
    "\n",
    "            for j, other_phrase in enumerate(combined_phrases):\n",
    "                if i != j and j not in skip:\n",
    "                    other_phrase_words = other_phrase.split()\n",
    "\n",
    "                    if phrase_words[:similarity_threshold] == other_phrase_words[:similarity_threshold]:\n",
    "                        combined_phrase = ' '.join(phrase_words + other_phrase_words[similarity_threshold:])\n",
    "                        skip.add(j)\n",
    "                        merged = True\n",
    "                    elif phrase_words[-similarity_threshold:] == other_phrase_words[-similarity_threshold:]:\n",
    "                        combined_phrase = ' '.join(phrase_words[:-similarity_threshold] + other_phrase_words)\n",
    "                        skip.add(j)\n",
    "                        merged = True\n",
    "                    else:\n",
    "                        combined_phrase = phrase\n",
    "\n",
    "            new_phrases.append(combined_phrase)\n",
    "\n",
    "        combined_phrases = new_phrases\n",
    "\n",
    "    return set(combined_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45566fe2-1e2a-4a23-8117-1e4e7b0729f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_post_process(phrases):\n",
    "    \"\"\"Apply all post-processing steps.\"\"\"\n",
    "    phrases = remove_substrings(phrases)\n",
    "    phrases = [remove_custom_words(phrase, custom_words) for phrase in phrases]\n",
    "    phrases = combine_similar_phrases(phrases)\n",
    "    phrases = remove_common_only_phrases(phrases, stop_words)\n",
    "    phrases = remove_all_lowercase_phrases(phrases)\n",
    "    phrases = remove_grammar(phrases)\n",
    "    \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6d6e51c-3792-404a-aa68-61e80b3aba52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crowned Clown 84',\n",
       " 'King the Land',\n",
       " 'King of the Land',\n",
       " 'Flower of Evil',\n",
       " 'Our Beloved Summer',\n",
       " 'Strong Woman Do Bong Soon',\n",
       " 'Love To Hate You',\n",
       " 'Extraordinary Attorney Woo',\n",
       " 'Destined with you',\n",
       " 'Weightlifting Fairy Kim Bok',\n",
       " 'Kim Bok Joo',\n",
       " 'Move to Heaven',\n",
       " 'Crowned Clown',\n",
       " 'Red Sleeve',\n",
       " 'Welcome to Waikiki',\n",
       " 'Hometown Cha Cha Cha',\n",
       " 'Moon Lovers Scarlet Heart',\n",
       " 'Crash Landing on You',\n",
       " 'My Love from',\n",
       " 'Business Proposal',\n",
       " 'Good Bad Mother']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_words = {'netflix', 'viki', 'mdl', 'rating', 'ml', 'fl'}\n",
    "stop_words = custom_stop_words\n",
    "\n",
    "common_phrases = extract_common_phrases(comments, ngram_limit=5, min_occurrences=2)\n",
    "common_phrases = post_process_ngrams(common_phrases)\n",
    "common_phrases = final_post_process(common_phrases)\n",
    "\n",
    "common_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce993d36-bdd0-4593-b294-6f1bd8b6ffb8",
   "metadata": {},
   "source": [
    "### 6. Compute Top 3 Recommendations using TF-IDF + Upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "030a0155-8f90-4ad2-9e70-12000e3c8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_tfidf(phrases, comments, ngram_limit=5):\n",
    "    comment_texts = [comment['text'] for comment in comments]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(vocabulary=phrases, ngram_range=(1, ngram_limit + 2), lowercase=False)\n",
    "    tfidf_matrix = vectorizer.fit_transform(comment_texts)\n",
    "\n",
    "    tfidf_scores = np.sum(tfidf_matrix.toarray(), axis=0)\n",
    "    phrase_tfidf_map = dict(zip(vectorizer.get_feature_names_out(), tfidf_scores))\n",
    "\n",
    "    return phrase_tfidf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b26b70b-bcde-46e2-a28a-2894f762b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phrase_upvotes(phrases, comments):\n",
    "    phrase_upvote_map = defaultdict(int)\n",
    "\n",
    "    for comment in comments:\n",
    "        comment_text_lower = comment['text'].lower()\n",
    "\n",
    "        for phrase in phrases:\n",
    "            if phrase.lower() in comment_text_lower:\n",
    "                phrase_upvote_map[phrase] += comment['score']\n",
    "\n",
    "    return phrase_upvote_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8062a440-953e-4343-a2b7-ee1983ff65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_phrases_combined(phrases, comments, top_n=10, ngram_limit=5):\n",
    "    phrase_tfidf_map = phrase_tfidf(phrases, comments, ngram_limit)\n",
    "    phrase_upvotes = compute_phrase_upvotes(phrases, comments)\n",
    "\n",
    "    combined_scores = {}\n",
    "    for phrase in phrases:\n",
    "        combined_score = phrase_tfidf_map.get(phrase, 0) + phrase_upvotes.get(phrase, 0)\n",
    "        combined_scores[phrase] = combined_score\n",
    "\n",
    "    sorted_phrases = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22a84f8f-b589-43a8-a141-0413fff78bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Crash Landing on You', np.float64(92.18636027299029)),\n",
       " ('Our Beloved Summer', np.float64(48.42416876922289)),\n",
       " ('Business Proposal', np.float64(31.2676855558996))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_phrases = top_phrases_combined(common_phrases, comments, top_n=3)\n",
    "top_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4914e30d-57e1-490b-acb5-7b2ccf816dd1",
   "metadata": {},
   "source": [
    "### 7. Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a03fc930-5280-400a-9913-3d34544c42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_reddit_phrases():\n",
    "    url = input('Please enter the Reddit URL: ')\n",
    "    url = url if url else 'https://www.reddit.com/r/kdramarecommends/comments/17ehrue/best_kdramas/'\n",
    "    \n",
    "    top_n = input('How many top phrases would you like to return? (default is 3): ')\n",
    "    top_n = int(top_n) if top_n.isdigit() else 3\n",
    "\n",
    "    custom_words_input = input('Enter custom words to exclude (comma separated, leave empty for none): ')\n",
    "    custom_words = set(custom_words_input.lower().split(',')) if custom_words_input else None\n",
    "\n",
    "    apply_remove_lowercase = input('Do you want to remove phrases that are all lowercase? (y/n, default is y): ').lower() != 'n'\n",
    "\n",
    "    ngram_limit = input('What is the maximum n-gram length? (default is 5): ')\n",
    "    ngram_limit = int(ngram_limit) if ngram_limit.isdigit() else 5\n",
    "\n",
    "    print_scores = input('Would you like to print scores? (y/n, default is n): ').lower() == 'y'\n",
    "\n",
    "    # Step 1: Fetch Reddit JSON\n",
    "    print('\\nStep (1/4): Fetching Reddit JSON data...')\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url + '.json', headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to fetch data from Reddit')\n",
    "        return None\n",
    "\n",
    "    data = response.json()\n",
    "    print('Done.\\n')\n",
    "\n",
    "    # Step 2: Extract and Clean Comments\n",
    "    print('Step (2/4): Extracting and cleaning comments...')\n",
    "    comments = []\n",
    "    extract_all_comments(data, comments)\n",
    "    \n",
    "    for comment in tqdm(comments, desc='Cleaning comments'):\n",
    "        comment['text'] = clean_text(comment['text'])\n",
    "    print('Done.\\n')\n",
    "\n",
    "    # Step 3: Extract Common Phrases\n",
    "    print('Step (3/4): Extracting common phrases...')\n",
    "    min_occurrences = max(math.ceil(len(comments) / 40), 2)\n",
    "    all_common_phrases = set()\n",
    "    all_common_phrases_lower = set()\n",
    "\n",
    "    while min_occurrences >= 2:\n",
    "        common_phrases = extract_common_phrases(comments, ngram_limit=ngram_limit, min_occurrences=min_occurrences)\n",
    "        common_phrases = post_process_ngrams(common_phrases)\n",
    "        common_phrases = final_post_process(common_phrases)\n",
    "        \n",
    "        if apply_remove_lowercase:\n",
    "            common_phrases = remove_all_lowercase_phrases(common_phrases)\n",
    "\n",
    "        new_phrases = set(phrase for phrase in common_phrases if phrase.lower() not in all_common_phrases_lower)\n",
    "        all_common_phrases.update(new_phrases)\n",
    "        all_common_phrases_lower.update(phrase.lower() for phrase in new_phrases)\n",
    "\n",
    "        print(f'Found {len(new_phrases)} unique phrases with min_occurrences={min_occurrences}. Total: {len(all_common_phrases)} phrases.')\n",
    "        \n",
    "        if len(all_common_phrases) >= top_n:\n",
    "            break\n",
    "\n",
    "        min_occurrences -= 1\n",
    "\n",
    "    if not all_common_phrases:\n",
    "        print('Comments are too varied or scattered. Unable to extract meaningful phrases.')\n",
    "        return None\n",
    "\n",
    "    print('Done.\\n')\n",
    "\n",
    "    # Step 4: Compute Top Phrases Using TF-IDF + Upvotes\n",
    "    print('Step (4/4): Calculating top phrases...')\n",
    "    top_phrases = top_phrases_combined(all_common_phrases, comments, top_n=top_n, ngram_limit=ngram_limit)\n",
    "    print('Done.\\n')\n",
    "\n",
    "    if len(top_phrases) < top_n:\n",
    "        print(f'\\nOnly {len(top_phrases)} common phrases found. Displaying top {len(top_phrases)} phrases.')\n",
    "\n",
    "    print(f'\\nTop {len(top_phrases)} Phrases:')\n",
    "    for idx, (phrase, score) in enumerate(top_phrases, 1):\n",
    "        if print_scores:\n",
    "            print(f'{idx}. {phrase}: {score}')\n",
    "        else:\n",
    "            print(f'{idx}. {phrase}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b69c4d2-a5b3-4b89-83d2-a19042a60438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the Reddit URL:  \n",
      "How many top phrases would you like to return? (default is 3):  10\n",
      "Enter custom words to exclude (comma separated, leave empty for none):  \n",
      "Do you want to remove phrases that are all lowercase? (y/n, default is y):  \n",
      "What is the maximum n-gram length? (default is 5):  \n",
      "Would you like to print scores? (y/n, default is n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step (1/4): Fetching Reddit JSON data...\n",
      "Done.\n",
      "\n",
      "Step (2/4): Extracting and cleaning comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning comments: 100%|████████████████████████████████████████████████████████| 88/88 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\n",
      "Step (3/4): Extracting common phrases...\n",
      "Found 5 unique phrases with min_occurrences=3. Total: 5 phrases.\n",
      "Found 16 unique phrases with min_occurrences=2. Total: 21 phrases.\n",
      "Done.\n",
      "\n",
      "Step (4/4): Calculating top phrases...\n",
      "Done.\n",
      "\n",
      "\n",
      "Top 10 Phrases:\n",
      "1. Crash Landing On You: 89.92065576911217\n",
      "2. Our Beloved Summer: 47.77858906503419\n",
      "3. Business Proposal: 32.30881968270583\n",
      "4. Strong Woman Do Bong Soon: 20.82337592780974\n",
      "5. Love To Hate You: 20.192954255432088\n",
      "6. Extraordinary Attorney Woo: 19.374665238438944\n",
      "7. Weightlifting Fairy Kim Bok: 17.98521795313214\n",
      "8. Kim Bok Joo: 17.297055972213187\n",
      "9. Flower of Evil: 14.102375543292046\n",
      "10. Destined with you: 12.414213562373096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_top_reddit_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401f64f-6c1a-4910-848c-635dfda8e082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
